<div align="center">

# ğŸš€ G-CORE-X1 GPU  
### ğŸ§  Multi-Core SIMD GPU Compute Architecture (RTL)

<img src="https://readme-typing-svg.demolab.com?font=Fira+Code&size=22&pause=900&color=00F7FF&center=true&vCenter=true&width=900&lines=Multi-Core+SIMD+GPU;RTL+Implemented+%7C+Simulated+%7C+Verified;Instruction-Driven+Parallel+Compute;Built+From+Scratch+in+Verilog;Waveform-Verified+Hardware+Design" />

</div>

---

## ğŸŸ¢ Project Status

```diff
+ RTL DESIGN COMPLETE
+ SIMD VECTOR EXECUTION VERIFIED
+ LOAD / STORE MEMORY IMPLEMENTED
+ MULTI-CORE GPU FUNCTIONAL
+ WAVEFORM-BASED VERIFICATION DONE


---
```

ğŸ“Œ Overview

G-CORE-X1 is a real GPU compute core, implemented at Register Transfer Level (RTL) using Verilog.

This project implements the compute engine of a GPU, focusing on parallel SIMD execution, not graphics output.

It is designed to demonstrate how a GPU works internally:

instruction fetch

vector execution

memory access

multi-core scaling


This is hardware. It is simulated, verified, and reproducible.


---

ğŸ¯ What This Project IS

âœ… A programmable SIMD GPU compute architecture
âœ… A parallel processor with multiple compute units
âœ… An RTL-level hardware design
âœ… Verified using industry-standard simulation
âœ… Suitable for academic, learning, and architecture exploration


---

ğŸš« What This Project is NOT

âŒ Not a graphics card
âŒ No HDMI / display output
âŒ No rasterizer or texture units
âŒ No transistor-level GDS or tape-out
âŒ No fake performance charts or FPS claims

```
---

ğŸ§© High-Level Architecture

+-----------------------------------------------------------+
|                       G-CORE-X1 GPU                       |
|                                                           |
|  +------------------ Shared Instruction Memory ----------+|
|  |                                                       ||
|  |   +-------------+       +-------------+              ||
|  |   | Compute CU0 |       | Compute CU1 |              ||
|  |   |-------------|       |-------------|              ||
|  |   | SIMD ALU    |       | SIMD ALU    |              ||
|  |   | Vector RF   |       | Vector RF   |              ||
|  |   | Data Memory |       | Data Memory |              ||
|  |   +-------------+       +-------------+              ||
|  |                                                       ||
|  +-------------------------------------------------------+|
|                                                           |
|        Global Interconnect + Clock Distribution Network   |
+-----------------------------------------------------------+


---
```
ğŸ§  GPU Design Layers

ğŸ”¹ Layer 0 â€” SIMD ALU

Multi-lane vector ALU

ADD and MUL operations

Same instruction applied across all lanes


ğŸ”¹ Layer 1 â€” Vector Register File

Multiple vector registers

Parallel read/write access

Supplies operands to ALU


ğŸ”¹ Layer 2 â€” Instruction Fetch

Program Counter (PC)

Instruction Memory (IMEM)

Autonomous execution (no manual driving)


ğŸ”¹ Layer 3 â€” Data Memory

Local per-core memory

LOAD and STORE instructions

Memory-based computation


ğŸ”¹ Layer 4 â€” Multi-Core GPU

Multiple compute units instantiated

Shared instruction stream

Parallel execution across cores



---

ğŸ“œ Instruction Set Architecture (ISA)

Instruction Format (16-bit)

[15:14] Opcode
[13:11] Destination Register
[10:8]  Source Register
[7:0]   Immediate / Address

Supported Instructions

Opcode	Instruction	Description

00	ADD	Vector addition
01	MUL	Vector multiplication
10	LOAD	Load vector from memory
11	STORE	Store vector to memory


Execution Model: SIMD
One instruction â†’ executed across all vector lanes.


---

ğŸ§ª Simulation & Verification

Verification is performed using RTL simulation, which is the industry-standard method for validating hardware designs.

Tools Used

Icarus Verilog

EDA Playground

EPWave (waveform viewer)


Verified Behavior

+ Program Counter increments correctly
+ Instructions fetched from IMEM
+ SIMD ALU executes vector operations
+ Register file write-back functions correctly
+ LOAD / STORE memory operations verified
+ Multiple compute units execute in parallel

ğŸ“ˆ Waveforms (.vcd) are the simulation graphs
They provide complete functional proof.


---

â–¶ï¸ How to Use / Run This GPU

Option 1 â€” Run on EDA Playground (Recommended)

1. Open https://edaplayground.com


2. Select:

Language: SystemVerilog / Verilog

Simulator: Icarus Verilog



3. Paste:

RTL into design.sv

Testbench into testbench.sv



4. Set Top Module:

tb_multi_core


5. Click Run


6. Open EPWave


7. Observe signals:

clk

pc

opcode

alu_out

mem_out

core0.pc

core1.pc




You are now watching a GPU execute instructions.


---

Option 2 â€” Run Locally (Linux / WSL)

iverilog -g2012 design.sv testbench.sv
vvp a.out
gtkwave dump.vcd


---

ğŸ§ª Example Instruction Program

// Example program in Instruction Memory
imem[0] = ADD   R3 = R1 + R2
imem[1] = MUL   R4 = R1 * R2
imem[2] = STORE R4 -> MEM[9]

This demonstrates:

Instruction fetch

SIMD execution

Register write-back

Memory store


```
---

ğŸ“‚ Repository Structure

G-CORE-X1-GPU/
â”‚
â”œâ”€â”€ rtl/                # Verilog RTL modules
â”‚   â”œâ”€â”€ simd_alu.v
â”‚   â”œâ”€â”€ vector_regfile.v
â”‚   â”œâ”€â”€ data_memory.v
â”‚   â”œâ”€â”€ compute_unit.v
â”‚   â””â”€â”€ multi_core_gpu.v
â”‚
â”œâ”€â”€ testbench/
â”‚   â””â”€â”€ tb_multi_core.sv
â”‚
â”œâ”€â”€ simulations/
â”‚   â””â”€â”€ *.vcd
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ diagrams/
â”‚
â””â”€â”€ README.md


---
```

ğŸ— Circuit-Level View (Conceptual)

Instruction Memory â†’ Control Logic

Control Logic â†’ Compute Units

Each Compute Unit contains:

SIMD ALU

Vector Register File

Local Data Memory



This is a block-level circuit schematic, standard in GPU documentation.


---

ğŸ— Physical Layout (Conceptual)

Floorplan-level organization

Tiled compute units

Shared instruction memory

Global interconnect and clock network


âš  This represents physical organization, not fabricated silicon.


---

âš  Limitations

No branch unit

No warp scheduler

No cache hierarchy

No graphics pipeline

No PCIe / host interface

No physical tape-out


These are deliberate exclusions, not flaws.


---

ğŸ”® Future Work

Warp / thread scheduler

Shared global memory

Cache hierarchy

Branch handling

Graphics front-end

Synthesis and P&R flow



---

â“ FAQ

Is this a real GPU?
Yes â€” a real GPU compute core, not a display GPU.

Does it execute instructions?
Yes â€” instruction-driven SIMD execution.

Is it verified?
Yes â€” via RTL simulation and waveforms.

Why no graphics output?
Graphics pipelines are separate subsystems.


---

ğŸ Final Statement

G-CORE-X1 is a real, working, multi-core SIMD GPU compute architecture, implemented and verified at RTL level.

No exaggeration.
No copied content.
No fake claims.

Just engineering.

